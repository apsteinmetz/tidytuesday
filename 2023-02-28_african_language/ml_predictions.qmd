---
title: "sentiment models"
format: html
editor: visual
---

Our purpose is twofold. First, it is interesting to ask whether we can build a better model for analyzing tweet sentiment in African languages by using native languages, many of them, or translate them all into one language first, English, using Google Translate.

[Paper describing data set](https://arxiv.org/pdf/2302.08956.pdf)

[African stopwords from Kaggle](https://www.kaggle.com/datasets/rtatman/stopword-lists-for-african-languages?resource=download)

Second, we want to explore using sparse data structures in the Tidy modeling framework. We will be using document term models which can generate giant, but mostly empty, matrices. Sparse data saves gobs of memory and boosts speed. As with many of my explorations, I start by seeing what [Julia Silge has to teach me about it](https://www.tidyverse.org/blog/2020/11/tidymodels-sparse-support/). One thing that is both frustrating and thrilling is finding out that I've been doing a lot of things the hard way and my code can be so much cleaner and faster.

```{r}
# topic modeling of sentiment
library(tidyverse)
library(tidytext)
library(tidymodels)
library(textrecipes)
# library(tm)
library(tictoc)
library(butcher)
library(yardstick)
library(quanteda)
library(hardhat)


# setwd("/2023-02-28_african_language")
load("data/afrisenti_translated.rdata")
```

Split into training and test sets based on designation already provided in data set. What is the reason for this split? I don't know.

```{r}
# ----- SETUP ------------------------------
afrisenti_translated <- afrisenti_translated %>%
  mutate(lang = as.factor(assigned_long)) %>%
  mutate(lang_iso = as.factor(assigned_language)) %>%
  mutate(sentiment = as.factor(label)) %>% 
  select(tweet_num,sentiment,lang,lang_iso,tweet,translatedText,intended_use)

summary(afrisenti_translated)
```

```{r}
tweet_train <- afrisenti_translated %>% 
  filter(intended_use == "train") %>% 
  select(tweet_num,sentiment,lang,tweet)

tweet_test <- afrisenti_translated %>% 
  filter(intended_use == "test") %>% 
  select(tweet_num,sentiment,lang,tweet)

tweet_dev <- afrisenti_translated %>% 
  filter(intended_use == "dev") %>% 
  select(tweet_num,sentiment,lang,tweet)

```

Let's see if the training set is representative of the test set.Do the languages align?

```{r}
afrisenti_translated %>% 
ggplot(aes(lang_iso,group=intended_use)) + 
   geom_bar(aes(y = after_stat(prop))) + 
          scale_y_continuous(labels=scales::percent) +
  theme(axis.text.x = element_text(angle = 45)) +
  facet_grid(~intended_use)
```

Looks okay.

Do the sentiments align?

```{r}
afrisenti_translated %>% 
ggplot(aes(sentiment,group=intended_use)) + 
   geom_bar(aes(y = after_stat(prop))) + 
          scale_y_continuous(labels=scales::percent) +
  theme(axis.text.x = element_text(angle = 45)) +
  facet_grid(~intended_use)

```

They look pretty balanced.

Now lets prepare the data. First we break each tweet into separate words, or tokens. To run a classifier model we basically associate the presence or absence of a word with a sentiment. That means every tweet is a row and every word in every tweet is a column indicating the number of times it appears in the tweet. This makes for an enormous matrix that takes up memory and a lot of time to scan. Since every word in the whole corpus is represented in every row we can assume the vast majority of cells have zero in them so it's a very sparse matrix. We have sparsity-aware tools that can make our job much more efficient.

::: callout-caution
Note that we don''t care what language the token is. It could be any language or no language. It could be an emoji, as long as it is associated with a sentiment. There is a risk that the same word could convey the opposite sentiment in two different languages but I assume it is rare enough to ignore.
:::

We can do slightly better than just noting how often a word appears in a tweet by computed each word's tf-idf.  I'll let you look up the details of this but it is basically a measure of the word's uniqueness in the corpus.

```{r}
tweet_words_af <-tweet_train %>%
  select(tweet_num,lang,tweet) %>% 
  # add language to every tweet if we can't bind a second dfm with just language
  # mutate(tweet = paste(tweet,lang)) %>% 
  unnest_tokens(word, tweet) %>%
  count(tweet_num, lang, word) %>%
  bind_tf_idf(word, tweet_num, n)

tweet_words_af
```

It is common in analyzing text to drop low-information words or "stop words."  In English, words like "the" and "that."  On Kaggle I found a list of stop words in various African languages.  It doesn't cover every language in our data set but will reduce the matrix size a bit.

```{r}
# path update needed in blog
load("~/R Projects/tidytuesday/2023-02-28_african_language/data/stopwords_af.rdata")

tweet_words_af <- tweet_words_af %>% 
  anti_join(stopwords_af, by = "lang")

tweet_words_af
```
Wow! Throwing out words we know won't help cuts down the data set size by about 40%!

Suppose we don't have a list of stop words for our langauge of interest? One of the benefits of the tf-idf score is stop words will naturally have a very low score because of their common-ness.

Finally, we convert this list of tokens into te sparse matrix our model will use.
```{r}
sparse_tweet_af <- tweet_words_af %>%
  cast_dfm(tweet_num, word, tf_idf)

sparse_tweet_af
```


Also, I would like the language itself to be a feature.  Maybe the speakers of a certain language use Twitter just to complain.  Let's make a DFM for just language.  We don't use TF-IDF, just whether the language is used, or not. 
```{r}
sparse_tweet_af <- tweet_words_af %>%
  mutate(lang = as.character(lang)) %>% 
  cast_dfm(tweet_num,lang,value=n) %>% 
  cbind(sparse_tweet_af)

sparse_tweet_af
```

Now we have our feature matrix, ready to process.  It takes up mere megabytes rather than gigabytes for a non-sparse matrix.

Set up the model
```{r}

```



Now let's to this the tidymodels way
```{r}
# this sets up up to handle the  sparse DFM
# I confess I'm just copy/pasting Julia's code without full understanding of the hardhat package
sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")


tweet_rec_af <-
  recipe(sentiment ~ tweet, data = tweet_train) %>%
  step_tokenize(tweet)  %>%
  # this is flawed given that a stopword in any language is
  # is considered a stop word in all languages
  # but the filter and tfidf also address the stopword problem
  # step_stopwords(tweet,custom_stopword_source = stopwords_af$stopword) %>%
  # use top 10000 tokens. This may be too small given multiple languages
  step_tokenfilter(tweet, max_tokens = 1e4) %>%
  step_tfidf(tweet)

```

Set up a couple models to try.  Both are sparse-aware but not all parsnip models are
```{r}
lasso_spec <-
  logistic_reg(penalty = 0.02, mixture = 1) %>%
  set_engine("glmnet")

xg_spec <-
  boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
```


Set up the workflow
```{r}
wf_sparse <- 
  workflow() %>%
  add_recipe(text_rec, blueprint = sparse_bp) %>%
  add_model(xg_spec)
```

```{r}
tweet_model_af <- fit(wf_sparse,tweet_train)
tweet_pred_af <- predict(tweet_model_af,tweet_train)
summary(tweet_test)
summary(tweet_pred_af)
```

